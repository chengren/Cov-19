install.packages('BSDA')
library(BSDA)#for z.test
z.test(df$meanvisit)
?z.test
z.test(df$meanvisit,sigma.x=sd(df$meanvisit))
#get 95% without group difference
t.test(df$meanvisit)
z.test(df$meanvisit,sigma.x=sd(df$meanvisit))
z.test(df[df$group==0,]$meanvisit,sigma.x=sd(df[df$group==0,]$meanvisit))
z.test(df[df$group==1,]$meanvisit,sigma.x=sd(df[df$group==1,]$meanvisit))
t.test(df[df$group==1,]$meanvisit)
#get 95% with group difference
t.test(df[df$group==0,]$meanvisit)
z.test(df[df$group==0,]$meanvisit,sigma.x=sd(df[df$group==0,]$meanvisit))
t.test(df[df$group==1,]$meanvisit)
z.test(df[df$group==1,]$meanvisit,sigma.x=sd(df[df$group==1,]$meanvisit))
library(tidycensus)
library(tidyr)
library(dplyr)
library(sf)
library(stringr)
# SET WD
setwd("/Users/chengren/Documents/GitHub/Cov-19")
#Set Options
options(tigris_class = "sf")
options(tigris_use_cache = TRUE)
# Load census key - Patty's key, apply for your own at:
my_census_api_key <- "f2d6f4f743545d3a42a67412b05935dc7712c432"
# Make tidycensus aware of census key
census_api_key(my_census_api_key)
# Identify county or counties of interest
#my_counties <- c("001", "075", "013", "041", "055", "081", "085", "095", "097")
# Alameda, SF, Contra Costa, Marin County, Napa,
# San Mateo, Santa Clara,  Solano,  Sonoma,
# Removing santa cruz ("087"")
states <- c('AL','AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'DC', 'FL', 'GA')
states1<- c('HI','IA', 'KS', 'KY', 'LA', 'ME', 'MD')
states2 <- c('MA', 'MI',  'MN', 'MS', 'MO', 'MT', 'NE', 'NV','NH')
states3 <- c('NJ', 'NM', 'NY', 'NC', 'ND','OH', 'OK', 'OR', 'PA')
states4 <- c('PR','RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA','WA', 'WV', 'WI', 'WY')
# Load cenvar lookup table of vars of interest
#my_cenvar_df <-read.csv("msi_acs_var_lookup.csv", strip.white = T, stringsAsFactors = F)
my_cenvar_df <-read.csv("Data_input/msi_cenvars_lut.csv", strip.white = T, stringsAsFactors = F)
# Fetch the ACS data
ct_data <- get_acs(geography = "county",
variables = my_cenvar_df$my_cen_vars,
year=2018,
survey="acs5",
state = states,
#county = my_counties,
geometry = T,
keep_geo_vars = T)
ct_data1 <- get_acs(geography = "county",
variables = my_cenvar_df$my_cen_vars,
year=2018,
survey="acs5",
state = states1,
#county = my_counties,
geometry = T,
keep_geo_vars = T)
ct_data2 <- get_acs(geography = "county",
variables = my_cenvar_df$my_cen_vars,
year=2018,
survey="acs5",
state = states2,
#county = my_counties,
geometry = T,
keep_geo_vars = T)
ct_data3 <- get_acs(geography = "county",
variables = my_cenvar_df$my_cen_vars,
year=2018,
survey="acs5",
state = states3,
#county = my_counties,
geometry = T,
keep_geo_vars = T)
ct_data4 <- get_acs(geography = "county",
variables = my_cenvar_df$my_cen_vars,
year=2018,
survey="acs5",
state = states4,
#county = my_counties,
geometry = T,
keep_geo_vars = T)
ct_data5 <- get_acs(geography = "county",
variables = my_cenvar_df$my_cen_vars,
year=2018,
survey="acs5",
state = c('ID','IL'),
#county = my_counties,
geometry = T,
keep_geo_vars = T)# do not know why 'ID'/ILis different from others
ct_data6 <- get_acs(geography = "county",
variables = my_cenvar_df$my_cen_vars,
year=2018,
survey="acs5",
state = c('IN'),
#county = my_counties,
geometry = T,
keep_geo_vars = T)#
# Reformat the data table so that
# (1) we only keep the columns of interest - including NAME, GEOID and geometry
# (2) each estimate variable is in its own column
# ORDER MATTERS - infile must have cenvars in cenvar order!!!!
ct_data_c <- rbind(ct_data,ct_data1,
ct_data2,ct_data3,
ct_data4,ct_data5,
ct_data6)
#tract_data_c <- rbind(tract_data,tract_data1,
#                      tract_data2,tract_data3,
#                      tract_data4,tract_data5,
#                      tract_data6)
tract_data_c1 <- ct_data_c %>%
select("NAME.y","GEOID","ALAND","variable","estimate") %>%
spread(key=variable, value=estimate)
#Remove rows with no land are
tract_data_c1  <- tract_data_c1 [!is.na(tract_data_c1 $ALAND),]
# Copy for debugging
tract_data_c2 <- tract_data_c1
# rename columns
colnames(tract_data_c2) <- c("NAME","GEOID", "land_area",my_cenvar_df$my_cen_var_names,"geometry")
# Remove rows with no people
tract_data_c2 <- tract_data_c2[tract_data_c2$tract_totpop!=0,]
# Change units for land_area from sq meters to sq km
tract_data_c2$p_land_areakm2 <- tract_data_c2$land_area / (1000 * 1000)
tract_data_c2$p_land_acreage <- tract_data_c2$land_area / 4046.856
tract_data_c2$p_land_sqmiles <- tract_data_c2$land_area / 2589988.1103
#tfix <- tract_data2[tract_data2$GEOID == "06041122000",]
# Create Calculated columns
## Note: keeping colnames short due to shapefile limits
tract_data_c3 <- tract_data_c2 %>%
mutate (
# Immigrant noncitizens in the total pop
p_noncit_totpop = tract_totpop,
p_noncit_count = tract_fborn_noncit,
p_noncit = ifelse(tract_totpop==0, 0, round(100 * (tract_fborn_noncit/tract_totpop),1)),
p_noncit_density = ifelse(tract_totpop==0, 0, round((tract_fborn_noncit / p_land_sqmiles),1)),
# Immigrants in the total pop
p_fborn_totpop = tract_totpop,
p_fborn_count = tract_fborn,
p_fborn =   ifelse(tract_totpop==0, 0, round(100 * (tract_fborn / tract_totpop), 1)),
p_fborn_density =  ifelse(tract_totpop==0, 0, round((tract_fborn / p_land_sqmiles),1)),
# Recent Immigrants in the total pop
p_recent_totpop = tract_totpop,
p_recent_count = tract_post2010_fborn,
p_recent =  ifelse(tract_totpop==0, 0, round(100 * (tract_post2010_fborn / tract_totpop),1)),
p_recent_density = ifelse(tract_totpop==0, 0, round((tract_post2010_fborn / p_land_sqmiles),1)),
# Immigrants living 150% below poverty in the Population for whom poverty has been determined
p_fb_pov_totpop = poverty_totpop,
p_fb_pov_count = (poverty_100below_fborn + poverty_100to149below_fborn),
p_fb_pov =  ifelse(poverty_totpop==0, 0, round(100 * ((poverty_100below_fborn + poverty_100to149below_fborn) / poverty_totpop),1)),
p_fb_pov_density =  ifelse(poverty_totpop==0, 0, round( ((poverty_100below_fborn + poverty_100to149below_fborn) / p_land_sqmiles),1)),
# Immigrants with no health insurance in the Civilian Noninstitutionalized Population
p_fb_nohi_totpop = nohealth_totpop,
p_fb_nohi_count = (nohealth_fborn_nat + nohealth_fborn_noncit),
p_fb_nohi = ifelse(nohealth_totpop==0, 0, round(100 * (nohealth_fborn_nat + nohealth_fborn_noncit) / nohealth_totpop,1)),
p_fb_nohi_density = ifelse(nohealth_totpop==0, 0, round(((nohealth_fborn_nat + nohealth_fborn_noncit) / p_land_sqmiles),1)),
#
p_fb_noncit_nohi_totpop = nohealth_totpop,
p_fb_noncit_nohi_count = nohealth_fborn_noncit,
p_fb_noncit_nohi = ifelse(nohealth_totpop==0, 0, round(100 * (nohealth_fborn_noncit / nohealth_totpop),1)),
p_fb_noncit_nohi_density =  ifelse(nohealth_totpop==0, 0, round((nohealth_fborn_noncit / p_land_sqmiles),1)),
# Limited english speakers by language spoken at home in the population over 5
# limited_english_totpop variable is the denominator
p_lim_eng_totpop = limited_english_totpop,
#spanish
p_lim_eng_spanish_count = limited_english_spanish,
p_lim_eng_spanish = ifelse(limited_english_totpop==0, 0, round(100 * (limited_english_spanish / limited_english_totpop),1)),
p_lim_eng_spanish_density = ifelse(limited_english_totpop==0, 0, round((limited_english_spanish / p_land_sqmiles),1)),
#chinese
p_lim_eng_chinese_count = limited_english_chinese,
p_lim_eng_chinese = ifelse(limited_english_totpop==0, 0, round(100 * (limited_english_chinese / limited_english_totpop),1)),
p_lim_eng_chinese_density = ifelse(limited_english_totpop==0, 0, round((limited_english_chinese / p_land_sqmiles),1)),
#tagalog
p_lim_eng_tagalog_count = limited_english_tagalog,
p_lim_eng_tagalog = ifelse(limited_english_totpop==0, 0, round(100 * (limited_english_tagalog / limited_english_totpop),1)),
p_lim_eng_tagalog_density = ifelse(limited_english_totpop==0, 0, round((limited_english_tagalog / p_land_sqmiles),1)),
#vitenamese
p_lim_eng_vietnamese_count = limited_english_vietnamese,
p_lim_eng_vietnamese = ifelse(limited_english_totpop==0, 0, round(100 * (limited_english_vietnamese / limited_english_totpop),1)),
p_lim_eng_vietnamese_density = ifelse(limited_english_totpop==0, 0, round((limited_english_vietnamese / p_land_sqmiles),1)),
#korean
p_lim_eng_korean_count = limited_english_korean,
p_lim_eng_korean = ifelse(limited_english_totpop==0, 0, round(100 * (limited_english_korean / limited_english_totpop),1)),
p_lim_eng_korean_density = ifelse(limited_english_totpop==0, 0, round((limited_english_korean / p_land_sqmiles),1)),
#other - needed for determining quantile bins only
p_lim_eng_allgroups_count = (limited_english_spanish + limited_english_chinese + limited_english_tagalog +
limited_english_vietnamese + limited_english_korean +
limited_english_french +  limited_english_german + limited_english_russian +
limited_english_other_indoeuropean +  limited_english_other_api +
limited_english_arabic +  limited_english_other),
p_lim_eng_allgroups = ifelse(limited_english_totpop==0, 0, round(100 * (p_lim_eng_allgroups_count / limited_english_totpop),1))
)
# Transforming CRS from NAD83 to WGS84 ("+proj=longlat +datum=WGS84")
tract_data_c3 <- st_transform(tract_data_c3, 4326)
# WRITE output to CSV
write_sf(tract_data_c3,"msi_countydata_acs2018.csv", layer_options = "GEOMETRY=AS_WKT", delete_layer=TRUE, delete_dsn=TRUE)
# Save vars to be mapped only
county_data <- select(tract_data_c3, "NAME", "GEOID", starts_with("p_"))
county_data$state=str_split_fixed(county_data$NAME, ", ",2)[,2]
names(county_data$state)
length(unique(county_data$state))
unique(county_data$state)
# Remove temp objects
keep_objects <- c("county_data")
rm(list = setdiff(ls(), keep_objects))
# Write Census Data to an Rdata file (census)
save.image("county_data_processed.Rdata")
library(dplyr)
library(tidyr)
library(sf)
library(readxl)
library(tools)
setwd("/Users/chengren/Documents/GitHub/Cov-19/")
# Source code to jitter any duplicate coords (locations)
source("0_geoR_jitter.R")
setwd("/Users/chengren/Documents/GitHub/Cov-19/Data_input")
df <- read.csv('Hospitals.csv')
# only for migrant
#df_mg <- df%>%filter(Migrant.Health.Centers.HRSA.Grant.Subprogram.Indicator == "Y")
hospitals <- df
hospitals$id <- 1:nrow(hospitals)
hospitals$NAME <-  toTitleCase(tolower(as.character(hospitals$NAME)))
hospitals$NAME <-  gsub("\\bUcsf","UCSF",hospitals$NAME)
hospitals$TYPE <-  toTitleCase(tolower(as.character(hospitals$TYPE)))
hospitals[c("X","Y")] <- jitterDupCoords(hospitals[c("X","Y")],min=0.00001, max=0.00009)
hospitals$ADDRESS<-  toTitleCase(tolower(as.character(hospitals$ADDRESS)))
hospitals$CITY<-  toTitleCase(tolower(as.character(hospitals$CITY)))
hospitals$full_address <- paste0(hospitals$ADDRESS, ", ",
hospitals$CITY, ", ",
hospitals$STATE, " ",
hospitals$ZIP)
hospitals$full_address<-  toTitleCase(as.character(hospitals$full_address))
###################################
# Reformat to create website URL
####################################
hospitals$website <- hospitals$WEBSITE
hospitals$website <- gsub('None|NA|N/A|NOT AVAILABLE','',hospitals$website)
hospitals$website <- ifelse(substring(hospitals$website, 1, 4) == "http"|
hospitals$website=="",
hospitals$website, paste0("http://", hospitals$website))
###################################
# telephone
####################################
hospitals$telephone <- hospitals$TELEPHONE
hospitals$telephone <- gsub('None|NA|N/A|NOT AVAILABLE','',hospitals$telephone)
###################################
# Format Zip Codes
#####################################
hospitals$zipcode <- substr(hospitals$ZIP, 1, 5)
health_keep_cols <- c(
"id",                      # need a unique id
"NAME",
"full_address",
"telephone",
#"Federally.Qualified.Health.Center.FQHC.Look.Alike.Organization.Site.Administrator.Contact.Email.Address",
"website",
#"Mission.Statement",
"X",
"Y",
#"health_services",
#"language_filter",
#"service_access",
#"Site.City",
"zipcode",
# "Migrant.Health.Centers.HRSA.Grant.Subprogram.Indicator",
# "Serving.Uninsured",
# "Capacity.Hours.of.Operation.per.Week"
"BEDS",
"TYPE"
)
hospitals <- hospitals[c(health_keep_cols)]
# Rename the columns
colnames(hospitals) <- c("ID", "org", "address", "phone", "website", #"mission",
"lon", "lat", #"services", "language_support", "service_access", #"city",
"zipcode","beds_capacity","type"
#"HRSA", "uninsured", "capacity_hours"
)
round(colSums(is.na(hospitals))/nrow(hospitals)*100,2)
#drop lon is NA
hospitals <- hospitals[!is.na(hospitals$lon), ]
round(colSums(is.na(hospitals))/nrow(hospitals)*100,2)
#write out
View(hospitals)
library(dplyr)
library(tidyr)
library(sf)
library(readxl)
library(tools)
setwd("/Users/chengren/Documents/GitHub/Cov-19")
# Source code to jitter any duplicate coords (locations)
source("0_geoR_jitter.R")
setwd("/Users/chengren/Documents/GitHub/Cov-19/Data_input")
df <- read.csv('SITE_HCC_FCT_DET.csv')
# only for migrant
#df_mg <- df%>%filter(Migrant.Health.Centers.HRSA.Grant.Subprogram.Indicator == "Y")
health_clinics <- df
health_clinics$ID <- 1:nrow(health_clinics)
health_clinics$Site.Name <- toTitleCase(tolower(as.character(health_clinics$Site.Name)))
health_clinics$Site.Name <- gsub('Gvhc|Ghvc', 'GVHC', health_clinics$Site.Name)
health_clinics$Site.Name <- gsub('Csvs', 'CSVS', health_clinics$Site.Name)
health_clinics$Site.Name <- gsub('Chcrr', 'CHCRR', health_clinics$Site.Name)
colnames(health_clinics) <- gsub("\\.\\.\\.\\.|\\.\\.", ".", colnames(health_clinics))
health_clinics[c("Geocoding.Artifact.Address.Primary.X.Coordinate","Geocoding.Artifact.Address.Primary.Y.Coordinate")] <- jitterDupCoords(health_clinics[c("Geocoding.Artifact.Address.Primary.X.Coordinate",
"Geocoding.Artifact.Address.Primary.Y.Coordinate")],
min=0.00001, max=0.00009)
health_clinics$full_address <- paste0(health_clinics$Site.Address, ", ",
health_clinics$Site.City, ", ",
health_clinics$Site.State.Abbreviation, " ",
health_clinics$Site.Postal.Code)
###################################
# Reformat to create website URL
####################################
health_clinics$website <- health_clinics$Site.Web.Address
health_clinics$website <- gsub('None|NA|N/A','',health_clinics$website)
health_clinics$website <- ifelse(substring(health_clinics$website, 1, 4) == "http"|
health_clinics$website=="",
health_clinics$website, paste0("http://", health_clinics$website))
health_clinics$website <- tolower(health_clinics$website)
health_clinics$website[grep("\\bGVHC",health_clinics$Site.Name)]<- "http://www.gvhc.org"
health_clinics$website[grep("\\bnot apply|\\bnot applicable",health_clinics$website)]<- ""
# Format Zip Codes
health_clinics$zipcode <- substr(health_clinics$Site.Postal.Code, 1, 5)
#get county code
table(health_clinics$County.or.County.Equivalent.Federal.Information.Processing.Standard.Code)
health_keep_cols <- c(
"ID",                      # need a unique id
"Site.Name",
"full_address",
"Site.Telephone.Number",
"Federally.Qualified.Health.Center.FQHC.Look.Alike.Organization.Site.Administrator.Contact.Email.Address",
"website",
#"Mission.Statement",
"Geocoding.Artifact.Address.Primary.X.Coordinate",
"Geocoding.Artifact.Address.Primary.Y.Coordinate",
#"health_services",
#"language_filter",
#"service_access",
#"Site.City",
"zipcode",
# "Migrant.Health.Centers.HRSA.Grant.Subprogram.Indicator",
# "Serving.Uninsured",
"Operating.Hours.per.Week"
)
health_clinics <- health_clinics[c(health_keep_cols)]
# Rename the columns
colnames(health_clinics) <- c("ID", "org", "address", "phone", "email", "website", #"mission",
"lon", "lat", #"services", "language_support", "service_access", #"city",
"zipcode","capacity_hours"
#"HRSA", "uninsured",
)
round(colSums(is.na(health_clinics))/nrow(health_clinics)*100,2)
#drop lon is NA
health_clinics <- health_clinics[!is.na(health_clinics$lon), ]
round(colSums(is.na(health_clinics))/nrow(health_clinics)*100,2)
#clinics_preload <- health_clinics
#clinics_preload$type <- "health"
#write out
View(health_clinics)
View(health_clinics)
shiny::runApp('~/Documents/GitHub/bimi_msi/COVID')
library(dplyr)
library(tidyr)
library(sf)
library(readxl)
library(tools)
setwd("/Users/chengren/Documents/GitHub/Cov-19")
# Source code to jitter any duplicate coords (locations)
source("0_geoR_jitter.R")
setwd("/Users/chengren/Documents/GitHub/Cov-19/Data_input")
df <- read.csv('SITE_HCC_FCT_DET.csv')
# only for migrant
#df_mg <- df%>%filter(Migrant.Health.Centers.HRSA.Grant.Subprogram.Indicator == "Y")
health_clinics <- df
health_clinics$ID <- 1:nrow(health_clinics)
health_clinics$Site.Name <- toTitleCase(tolower(as.character(health_clinics$Site.Name)))
health_clinics$Site.Name <- gsub('Gvhc|Ghvc', 'GVHC', health_clinics$Site.Name)
health_clinics$Site.Name <- gsub('Csvs', 'CSVS', health_clinics$Site.Name)
health_clinics$Site.Name <- gsub('Chcrr', 'CHCRR', health_clinics$Site.Name)
colnames(health_clinics) <- gsub("\\.\\.\\.\\.|\\.\\.", ".", colnames(health_clinics))
health_clinics[c("Geocoding.Artifact.Address.Primary.X.Coordinate","Geocoding.Artifact.Address.Primary.Y.Coordinate")] <- jitterDupCoords(health_clinics[c("Geocoding.Artifact.Address.Primary.X.Coordinate",
"Geocoding.Artifact.Address.Primary.Y.Coordinate")],
min=0.00001, max=0.00009)
health_clinics$full_address <- paste0(health_clinics$Site.Address, ", ",
health_clinics$Site.City, ", ",
health_clinics$Site.State.Abbreviation, " ",
health_clinics$Site.Postal.Code)
###################################
# Reformat to create website URL
####################################
health_clinics$website <- health_clinics$Site.Web.Address
health_clinics$website <- gsub('None|NA|N/A','',health_clinics$website)
health_clinics$website <- ifelse(substring(health_clinics$website, 1, 4) == "http"|
health_clinics$website=="",
health_clinics$website, paste0("http://", health_clinics$website))
health_clinics$website <- tolower(health_clinics$website)
health_clinics$website[grep("\\bGVHC",health_clinics$Site.Name)]<- "http://www.gvhc.org"
health_clinics$website[grep("\\bnot apply|\\bnot applicable",health_clinics$website)]<- ""
# Format Zip Codes
health_clinics$zipcode <- substr(health_clinics$Site.Postal.Code, 1, 5)
#get county code
table(health_clinics$County.or.County.Equivalent.Federal.Information.Processing.Standard.Code)
health_keep_cols <- c(
"ID",                      # need a unique id
"Site.Name",
"full_address",
"Site.Telephone.Number",
"Federally.Qualified.Health.Center.FQHC.Look.Alike.Organization.Site.Administrator.Contact.Email.Address",
"website",
#"Mission.Statement",
"Geocoding.Artifact.Address.Primary.X.Coordinate",
"Geocoding.Artifact.Address.Primary.Y.Coordinate",
#"health_services",
#"language_filter",
#"service_access",
#"Site.City",
"zipcode",
# "Migrant.Health.Centers.HRSA.Grant.Subprogram.Indicator",
# "Serving.Uninsured",
"Operating.Hours.per.Week"
)
health_clinics <- health_clinics[c(health_keep_cols)]
# Rename the columns
colnames(health_clinics) <- c("ID", "org", "address", "phone", "email", "website", #"mission",
"lon", "lat", #"services", "language_support", "service_access", #"city",
"zipcode","capacity_hours"
#"HRSA", "uninsured",
)
round(colSums(is.na(health_clinics))/nrow(health_clinics)*100,2)
#drop lon is NA
health_clinics <- health_clinics[!is.na(health_clinics$lon), ]
round(colSums(is.na(health_clinics))/nrow(health_clinics)*100,2)
#clinics_preload <- health_clinics
#clinics_preload$type <- "health"
#write out
setwd("/Users/chengren/Documents/GitHub/Cov-19/Data_output")
write.csv(health_clinics,"1_health_clinics_national.csv",row.names = FALSE)
View(health_clinics)
View(health_clinics)
View(health_clinics)
setwd("/Users/chengren/Documents/GitHub/Cov-19/")
# Source code to jitter any duplicate coords (locations)
source("0_geoR_jitter.R")
setwd("/Users/chengren/Documents/GitHub/Cov-19/Data_input")
df <- read.csv('Hospitals.csv')
# only for migrant
#df_mg <- df%>%filter(Migrant.Health.Centers.HRSA.Grant.Subprogram.Indicator == "Y")
hospitals <- df
hospitals$id <- 1:nrow(hospitals)
hospitals$NAME <-  toTitleCase(tolower(as.character(hospitals$NAME)))
hospitals$NAME <-  gsub("\\bUcsf","UCSF",hospitals$NAME)
hospitals$TYPE <-  toTitleCase(tolower(as.character(hospitals$TYPE)))
hospitals[c("X","Y")] <- jitterDupCoords(hospitals[c("X","Y")],min=0.00001, max=0.00009)
hospitals$ADDRESS<-  toTitleCase(tolower(as.character(hospitals$ADDRESS)))
hospitals$CITY<-  toTitleCase(tolower(as.character(hospitals$CITY)))
hospitals$full_address <- paste0(hospitals$ADDRESS, ", ",
hospitals$CITY, ", ",
hospitals$STATE, " ",
hospitals$ZIP)
hospitals$full_address<-  toTitleCase(as.character(hospitals$full_address))
###################################
# Reformat to create website URL
####################################
hospitals$website <- hospitals$WEBSITE
hospitals$website <- gsub('None|NA|N/A|NOT AVAILABLE','',hospitals$website)
hospitals$website <- ifelse(substring(hospitals$website, 1, 4) == "http"|
hospitals$website=="",
hospitals$website, paste0("http://", hospitals$website))
###################################
# telephone
####################################
hospitals$telephone <- hospitals$TELEPHONE
hospitals$telephone <- gsub('None|NA|N/A|NOT AVAILABLE','',hospitals$telephone)
###################################
# Format Zip Codes
#####################################
hospitals$zipcode <- substr(hospitals$ZIP, 1, 5)
health_keep_cols <- c(
"id",                      # need a unique id
"NAME",
"full_address",
"telephone",
#"Federally.Qualified.Health.Center.FQHC.Look.Alike.Organization.Site.Administrator.Contact.Email.Address",
"website",
#"Mission.Statement",
"X",
"Y",
#"health_services",
#"language_filter",
#"service_access",
#"Site.City",
"zipcode",
# "Migrant.Health.Centers.HRSA.Grant.Subprogram.Indicator",
# "Serving.Uninsured",
# "Capacity.Hours.of.Operation.per.Week"
"BEDS",
"TYPE"
)
hospitals <- hospitals[c(health_keep_cols)]
# Rename the columns
colnames(hospitals) <- c("ID", "org", "address", "phone", "website", #"mission",
"lon", "lat", #"services", "language_support", "service_access", #"city",
"zipcode","beds_capacity","type"
#"HRSA", "uninsured", "capacity_hours"
)
round(colSums(is.na(hospitals))/nrow(hospitals)*100,2)
#drop lon is NA
hospitals <- hospitals[!is.na(hospitals$lon), ]
round(colSums(is.na(hospitals))/nrow(hospitals)*100,2)
#write out
setwd("/Users/chengren/Documents/GitHub/Cov-19/Data_output")
write.csv(hospitals,"3_hospitals_national.csv",row.names = FALSE)
